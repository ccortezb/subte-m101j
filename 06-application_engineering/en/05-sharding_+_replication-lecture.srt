1
00:00:00,000 --> 00:00:00,260

2
00:00:00,260 --> 00:00:03,800
Sharding and replication are
almost always done together.

3
00:00:03,800 --> 00:00:06,200
So whenever you've got a
sharded environment--

4
00:00:06,200 --> 00:00:11,170
here I'm showing three shards,
s0, s1, and s2, and then a

5
00:00:11,170 --> 00:00:12,430
mongos routing them.

6
00:00:12,430 --> 00:00:14,890
The shards themselves
are replica sets.

7
00:00:14,890 --> 00:00:16,350
And they're replica sets,
because otherwise, they

8
00:00:16,350 --> 00:00:18,510
wouldn't be reliable.

9
00:00:18,510 --> 00:00:20,400
You just have to expect that
when you're dealing with a

10
00:00:20,400 --> 00:00:21,240
sharded environment.

11
00:00:21,240 --> 00:00:23,377
And best practices dictate
that that's the way you

12
00:00:23,377 --> 00:00:24,790
would set it up.

13
00:00:24,790 --> 00:00:28,180
Next thing I wanted to point
out is that the mongos is

14
00:00:28,180 --> 00:00:30,320
doing with the driver was doing
before in the sense that

15
00:00:30,320 --> 00:00:34,140
the mongos is reconnecting to
members of the replica set,

16
00:00:34,140 --> 00:00:36,160
because the mongos probably has
a connection all the way

17
00:00:36,160 --> 00:00:40,300
through to the primary node of
the replica set in each of

18
00:00:40,300 --> 00:00:43,950
these shards, and possibly
connections to the secondaries

19
00:00:43,950 --> 00:00:46,750
if it's allowing secondary
reads.

20
00:00:46,750 --> 00:00:50,320
So when there's failover within
a shard, it's the

21
00:00:50,320 --> 00:00:52,350
mongos that's going
to reconnect.

22
00:00:52,350 --> 00:00:55,340
And so the mongos has some
concept now of the seed list

23
00:00:55,340 --> 00:00:58,030
and knows which of these
nodes are part of it.

24
00:00:58,030 --> 00:01:00,980
The next thing that you should
realize is that there's still

25
00:01:00,980 --> 00:01:02,540
this concept of write concern.

26
00:01:02,540 --> 00:01:07,110
And there's still the j value
and w value and w timeout.

27
00:01:07,110 --> 00:01:09,010
And those are going to be
passed right through.

28
00:01:09,010 --> 00:01:12,890
So if your application specifies
that it requires

29
00:01:12,890 --> 00:01:17,030
journaling to be true, let's
say, or w majority, a majority

30
00:01:17,030 --> 00:01:20,310
of nodes need to see the write
before it's completed, then

31
00:01:20,310 --> 00:01:22,050
your application is going
to specify that.

32
00:01:22,050 --> 00:01:24,355
And the drivers are inside your
application or next to

33
00:01:24,355 --> 00:01:26,540
your application drivers.

34
00:01:26,540 --> 00:01:27,910
And that's going to
go to the mongos.

35
00:01:27,910 --> 00:01:30,840
The mongos is going to route the
query to the appropriate

36
00:01:30,840 --> 00:01:32,650
shard, the replica set.

37
00:01:32,650 --> 00:01:35,520
And then that j and that write
concern are going to get

38
00:01:35,520 --> 00:01:38,100
reflected in that final write.

39
00:01:38,100 --> 00:01:40,720
So you should continue to think
about having to specify

40
00:01:40,720 --> 00:01:43,600
those in the sense there's a
replicated environment here

41
00:01:43,600 --> 00:01:45,310
and that those are
still meaningful.

42
00:01:45,310 --> 00:01:49,950
And they're going to apply
to each of the nodes.

43
00:01:49,950 --> 00:01:53,240
So if hit a multi-update, it'll
hit multiple nodes, then

44
00:01:53,240 --> 00:01:55,770
in each node, if you said I
wanted the w value to be,

45
00:01:55,770 --> 00:02:01,290
let's say two, and j to one,
then this mongos wouldn't get

46
00:02:01,290 --> 00:02:03,640
an acknowledgement
for that write.

47
00:02:03,640 --> 00:02:06,720
It wouldn't complete it until
all the different nodes

48
00:02:06,720 --> 00:02:08,850
completed the write to
a majority of them.

49
00:02:08,850 --> 00:02:11,110
And then it was committed to the
journal on the primary of

50
00:02:11,110 --> 00:02:12,990
each of these replica sets.

51
00:02:12,990 --> 00:02:15,930
Those are some things to keep
in mind as you're working in

52
00:02:15,930 --> 00:02:16,940
this environment.

53
00:02:16,940 --> 00:02:19,180
And the final point I'll make
is that usually mongos is

54
00:02:19,180 --> 00:02:20,710
replicated itself.

55
00:02:20,710 --> 00:02:22,800
There's probably a few these
hanging out here.

56
00:02:22,800 --> 00:02:26,010
Typically, you run them on the
same box as the application

57
00:02:26,010 --> 00:02:28,720
server itself, because they're
pretty lightweight.

58
00:02:28,720 --> 00:02:32,190
And the driver is just the way
they can take multiple names

59
00:02:32,190 --> 00:02:35,740
of hosts for replica set, they
could take multiple mongoses.

60
00:02:35,740 --> 00:02:39,500
And it can't, for some reason,
attach to a mongos or connect

61
00:02:39,500 --> 00:02:43,550
to a mongos, then it will
connect to another mongos.

62
00:02:43,550 --> 00:02:46,490
So that's what I want to point
out to you about sharding plus

63
00:02:46,490 --> 00:02:48,340
replication together,
that you've got

64
00:02:48,340 --> 00:02:49,320
these replica set here.

65
00:02:49,320 --> 00:02:51,510
And you have these mongoses
in front.

66
00:02:51,510 --> 00:02:55,440
And everything you've learned
before about availability and

67
00:02:55,440 --> 00:02:57,110
replication still applies.

68
00:02:57,110 --> 00:03:00,670
It's just that it's passed
through to the various shards.

69
00:03:00,670 --> 00:03:01,850
Let's do a quiz.

70
00:03:01,850 --> 00:03:05,500
So suppose you want to run
multiple mongos routers for

71
00:03:05,500 --> 00:03:06,100
redundancy.

72
00:03:06,100 --> 00:03:09,170
What level of the stack will
assure that you can failover

73
00:03:09,170 --> 00:03:11,570
to a different mongos from
within your application.

74
00:03:11,570 --> 00:03:15,020
Is it the mongod, mongos, the
drivers, or the sharding

75
00:03:15,020 --> 00:03:16,270
config servers?

76
00:03:16,270 --> 00:03:17,000